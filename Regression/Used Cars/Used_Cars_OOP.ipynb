{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, train_file_path, test_file_path, target_var, num_features, cat_features):\n",
    "        '''creates training and test dataframes\n",
    "        cleans datasets before doing feature engineering\n",
    "        creates summary statistics and visualization of data as an EDA phase\n",
    "        '''\n",
    "        self.target_var = target_var\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        self.all_cols = num_features + cat_features + target_var\n",
    "        self.train_df = self._clean_data(train_file_path, corrupt=True, missing=True, outlier=True)\n",
    "        self.test_df = self._clean_data(test_file_path, corrupt=True, missing=True, outlier=False)\n",
    "        self.target_df = self.train_df[self.target_var]\n",
    "        \n",
    "    def target_summary(self):\n",
    "        print('\\nSummary of the target variable:')\n",
    "        print(self.train_df[target_var].info())\n",
    "        print(self.train_df[target_var].describe())\n",
    "    \n",
    "    def train_summary(self):\n",
    "        print('\\nSummary of the training file:')\n",
    "        train_cols = self.train_df.drop(columns=target_var).shape[1]\n",
    "        train_rows = self.train_df.shape[0]\n",
    "        print('\\nThe train file has {} features and {} data-points'.format(train_cols, train_rows))\n",
    "        print('\\nData types in each column:')\n",
    "        print(self.train_df.dtypes)\n",
    "        print('\\nNumber of missing values in each column:')\n",
    "        print(self.train_df.isna().sum())\n",
    "        print('\\nFirst 3 rows of the training data:')\n",
    "        print(self.train_df.head(3))\n",
    "        print('\\nNumber of unique values in each column:')\n",
    "        print(self.train_df.nunique())\n",
    "        print('\\nDescriptive stats of numerical features:')\n",
    "        print(self.train_df[[col for col in self.train_df.columns if self.train_df[col].dtype != 'object']].describe())\n",
    "        print('\\nDescriptive stats of categorical features:')\n",
    "        print(self.train_df[[col for col in self.train_df.columns if self.train_df[col].dtype == 'object']].describe())\n",
    "        \n",
    "    def test_summary(self):\n",
    "        print('\\nSummary of the test file:')\n",
    "        test_cols = self.test_df.shape[1]\n",
    "        test_rows = self.test_df.shape[0]\n",
    "        print('\\nThe test file has {} features and {} data-points'.format(test_cols, test_rows))\n",
    "        print('\\nData types in each column:')\n",
    "        print(self.test_df.dtypes)\n",
    "        print('\\nNumber of missing values in each column:')\n",
    "        print(self.test_df.isna().sum())\n",
    "        print('\\nFirst 3 rows of the test data:')\n",
    "        print(self.test_df.head(3))\n",
    "        print('\\nNumber of unique values in each columns:')\n",
    "        print(self.test_df.nunique())\n",
    "        print('\\nDescriptive stats of numerical features:')\n",
    "        print(self.test_df[[col for col in self.test_df.columns if self.test_df[col].dtype != 'object']].describe())\n",
    "        print('\\nDescriptive stats of categorical features:')\n",
    "        print(self.test_df[[col for col in self.test_df.columns if self.test_df[col].dtype == 'object']].describe())        \n",
    "        \n",
    "    def visualize_target_var(self):\n",
    "        #target variable\n",
    "        print('plotting the distribution of the target variable')\n",
    "        plt.figure(figsize = (14, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.boxplot(self.target_df)\n",
    "        plt.title(self.target_var[0])\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.distplot(self.target_df)\n",
    "        plt.title(self.target_var[0])\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_cat_features(self):\n",
    "        #categorical variables of the training set\n",
    "        print('plotting the distribution of categorical variables of the training data')\n",
    "        plt.figure(figsize=(14, 18))\n",
    "        i = 1\n",
    "        for col in self.cat_features:\n",
    "            if (col not in self.removed_cols) & (self.train_df[col].nunique() < 15):\n",
    "                plt.subplot(4, 2, i)\n",
    "                sns.countplot(self.train_df[col])\n",
    "                plt.title(col)\n",
    "                plt.xticks(rotation=90)\n",
    "                plt.tight_layout()                \n",
    "                \n",
    "                plt.subplot(4, 2, i+1)\n",
    "                sns.boxplot(x=col, y=self.target_var[0], data=self.train_df)\n",
    "                plt.title(col)\n",
    "                plt.xticks(rotation=90)\n",
    "                plt.tight_layout()\n",
    "                i += 2\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_num_features(self):\n",
    "        print('plotting the distribution of numerical variables of the training data')    \n",
    "        #numerical variables of the training set\n",
    "        plt.figure(figsize=(16, 7))\n",
    "        i = 1\n",
    "        for col in self.num_features:\n",
    "            if col not in self.removed_cols:\n",
    "                plt.subplot(3, 3, i)\n",
    "                sns.distplot(self.train_df[col])\n",
    "                plt.title(col)\n",
    "                plt.tight_layout()\n",
    "                i += 1\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_heatmap(self):\n",
    "        # heatmap\n",
    "        print('ploting heatmap to show correlations among variables')    \n",
    "        plt.figure(figsize = (7, 6))\n",
    "        sns.heatmap(self.train_df.corr(), cmap = 'Blues', annot = True, vmin = 0.2)\n",
    "        plt.show()\n",
    "    \n",
    "    def _clean_data(self, file_path, corrupt=True, missing=True, outlier=True):\n",
    "        df = self._load_data(file_path)\n",
    "        self.removed_cols = []\n",
    "        for col in df.columns:\n",
    "            if df[col].nunique()/len(df[col]) > 0.95:\n",
    "                df = df.drop(columns=[col])\n",
    "                self.removed_cols.append(col)\n",
    "        # fix corrupt data\n",
    "        if corrupt:\n",
    "            for col in ['Mileage', 'Engine', 'Power']:\n",
    "                col_split = df[col].str.split(' ', expand=True)\n",
    "                df[col] = col_split[0]\n",
    "            df.replace(['null', 0], np.nan, inplace=True)\n",
    "        # fix missing values\n",
    "        if missing:\n",
    "            df.dropna(thresh=5, inplace=True)\n",
    "            for col in df.columns:\n",
    "                if df[col].isna().sum()/len(df[col]) > 0.6:\n",
    "                    df = df.drop(columns=[col])\n",
    "                    self.removed_cols.append(col)\n",
    "            for col in self.num_features:\n",
    "                if col not in self.removed_cols:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "                    df[col].fillna(df[col].dropna().mean(), inplace=True)\n",
    "            for col in self.cat_features:\n",
    "                if col not in self.removed_cols:\n",
    "                    df[col].fillna(df[col].mode(), inplace=True)                             \n",
    "        # fix outlier data\n",
    "        if outlier:\n",
    "            for col in self.num_features:\n",
    "                if col not in self.removed_cols:\n",
    "                    pd.to_numeric(df[col])\n",
    "                    min_range = np.min((df[col].describe()['mean'] - 5 * df[col].describe()['std']), 0)\n",
    "                    max_range = df[col].describe()['mean'] + 5 * df[col].describe()['std']\n",
    "                    df = df[(df[col] > np.min(min_range, 0)) & (df[col] < max_range)]       \n",
    "        # return the cleaned dataframe\n",
    "        return df\n",
    "                \n",
    "    def _load_data(self, file_path):\n",
    "        return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    '''performs feature engineering nad featire selection for modeling'''\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        dummy_cols = []\n",
    "        for col in data.cat_features:\n",
    "            \n",
    "            if data.train_df[col].nunique()/len(data.train_df[col]) < 0.1:\n",
    "                for name, count in data.train_df[col].value_counts().items():\n",
    "                    if count/len(data.train_df[col]) < 0.01:\n",
    "                        data.train_df[col].replace(name, 'Rare', inplace=True)\n",
    "            if data.test_df[col].nunique()/len(data.test_df[col]) < 0.1:\n",
    "                for name, count in data.test_df[col].value_counts().items():\n",
    "                    if count/len(data.test_df[col]) < 0.01:\n",
    "                        data.test_df[col].replace(name, 'Rare', inplace=True)\n",
    "            \n",
    "            if data.train_df[col].nunique() > 10:\n",
    "                from category_encoders.target_encoder import TargetEncoder\n",
    "                encoder = TargetEncoder(cols=col)\n",
    "                encoder.fit(data.train_df[col], data.train_df[data.target_var])\n",
    "                data.train_df[col] = encoder.transform(data.train_df[col])\n",
    "                data.test_df[col] = encoder.transform(data.test_df[col])\n",
    "            else:\n",
    "                dummy_cols.append(col)\n",
    "        \n",
    "        data.train_df = pd.get_dummies(data.train_df, columns=dummy_cols, drop_first = True)\n",
    "        data.test_df = pd.get_dummies(data.test_df, columns=dummy_cols, drop_first = True)\n",
    "        data.target_df = data.train_df[data.target_var]\n",
    "        \n",
    "#         if scale:\n",
    "#             from sklearn.preprocessing import StandardScaler\n",
    "#             scaler = StandardScaler()\n",
    "#             data.train_df = pd.DataFrame(scaler.fit_transform(data.train_df), columns=data.train_df.columns)\n",
    "#             data.test_df = pd.DataFrame(scaler.fit_transform(data.test_df), columns=data.test_df.columns)\n",
    "#             self.scaler = scaler\n",
    "#             data.target_df = data.train_df[data.target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeling:\n",
    "    def __init__(self, models={}):   \n",
    "        self.models = models\n",
    "        \n",
    "    def add_model(self, model_name, model):\n",
    "        self.models[model_name] = model\n",
    "\n",
    "    def modeling_summary(self, data, k_cross_val=3, processor=-1, metric='neg_mean_squared_error'):\n",
    "        self.k_cross_val = k_cross_val\n",
    "        self.metric = metric\n",
    "        self.scores = {}\n",
    "        best_score = -99999\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "\n",
    "        for model_name, model in self.models.items():\n",
    "            score_list = cross_val_score(estimator=model, X=data.train_df.drop(columns=target_var), y=data.train_df[target_var],\n",
    "                                    cv=k_cross_val, n_jobs=processor, scoring=self.metric)\n",
    "            self.scores[model_name] = round(score_list.mean(), 3)\n",
    "            if score_list.mean() > best_score:\n",
    "                self.best_score = round(score_list.mean(), 3)\n",
    "                self.best_model = model\n",
    "                self.best_model_name = model_name\n",
    "        print(\"Here is the list of applied models and their '{}' scores:\\n {}\".format(self.metric, self.scores))\n",
    "        print(\"\\nThe best model was '{}' with '{}' score of {}\".format(self.best_model_name, self.metric, self.best_score))\n",
    "            \n",
    "    def get_feature_importance(self):\n",
    "        '''returns sorted features based on their importances,\n",
    "        which were calculated using RandomForest method\n",
    "        '''\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        features = data.train_df.drop(columns=target_var).columns\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(data.train_df.drop(columns=target_var), data.target_df)\n",
    "        importances = model.feature_importances_\n",
    "        feature_importances = pd.DataFrame({'Feature':features, 'Importance':importances})\n",
    "        feature_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "        feature_importances.set_index('Feature', inplace=True, drop=True)\n",
    "        return feature_importances\n",
    "    \n",
    "    def model_tuning(self, hyper_parameters):\n",
    "        self.hyper_parameters = hyper_parameters\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        grid_search = GridSearchCV(estimator=self.best_model,\n",
    "                                         param_grid=self.hyper_parameters,\n",
    "                                         scoring=self.metric,\n",
    "                                         n_jobs=-1,\n",
    "                                         cv=self.k_cross_val,\n",
    "                                         return_train_score=True)\n",
    "        grid_search.fit(X=data.train_df.drop(columns=target_var), y=data.target_df)\n",
    "        print('Best combination of hyperparameters are: {}'.format(grid_search.best_params_))\n",
    "        print(\"Best achieved '{}' score is: {}\".format(self.metric, round(grid_search.best_score_, 3)))\n",
    "        \n",
    "    def save_results(self, file_path):\n",
    "            self.best_model.fit(data.train_df.drop(columns=target_var), data.target_df)\n",
    "            predictions = self.best_model.predict(data.test_df)\n",
    "            predict_file = pd.DataFrame({'Car_ID': data.test_df.index, 'Car_Price': predictions})\n",
    "            predict_file.to_csv(file_path + '/Predicted_Price.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'C:/Users/mehdi/Downloads/used-cars-price-prediction/train-data.csv'\n",
    "test_file_path = 'C:/Users/mehdi/Downloads/used-cars-price-prediction/test-data.csv'\n",
    "target_var = ['Price']\n",
    "num_features = ['Year', 'Kilometers_Driven', 'Seats', 'Mileage', 'Engine', 'Power', 'New_Price']\n",
    "cat_features = ['Name', 'Location', 'Fuel_Type', 'Transmission', 'Owner_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(train_file_path, test_file_path, target_var, num_features, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_summary()\n",
    "# data.train_summary()\n",
    "# data.test_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.visualize_target_var()\n",
    "# data.visualize_num_features()\n",
    "# data.visualize_cat_features()\n",
    "# data.heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfeatures = FeatureEngineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = Modeling()\n",
    "\n",
    "# add regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "mymodel.add_model('lin_reg', LinearRegression())\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "mymodel.add_model('rand_forest', RandomForestRegressor(n_estimators=60, n_jobs=-1,\n",
    "                                        max_depth=15, min_samples_split=80,\n",
    "                                        max_features=8))\n",
    "\n",
    "mymodel.modeling_summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = [{'n_estimators': [40, 60, 100],\n",
    "                     'max_depth': [5, 15, 40],\n",
    "                     'min_samples_split': [40, 80, 100],\n",
    "                     'max_features': [5, 8, 11]\n",
    "                    }]\n",
    "mymodel.model_tuning(hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.save_results('C:/Users/mehdi/Downloads/used-cars-price-prediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
